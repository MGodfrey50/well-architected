---
title: Cloud design patterns that support performance efficiency
description: Learn about industry patterns that support performance efficiency and can help you address common challenges in cloud workloads.  
author: ckittel
ms.author: chkittel
ms.date: 11/15/2023
ms.topic: conceptual
---

# Cloud design patterns that support performance efficiency

When you design workload architectures, you should use industry patterns that address common challenges. Patterns can help you make intentional tradeoffs within workloads and optimize for your desired outcome. These patterns are backed by real-world experience, are designed for cloud scale and operating models, and are inherently vendor agnostic. Using well-known patterns as a way to standardize your workload design is a component of operational excellence.

Many design patterns directly support one or more architecture pillars. Design patterns that support the performance efficiency pillar address scalability, performance tuning, task prioritization, and removing bottlenecks.

## Design patterns for performance efficiency

The following table summarizes cloud design patterns that support the goals of performance efficiency.

|Pattern|Summary|
|-|-|
|[Asynchronous Request-Reply](/azure/architecture/patterns/async-request-reply)|Improves the responsiveness and scalability of systems by decoupling the request and reply phases of interactions for processes that don't need immediate answers. By using an asynchronous pattern, you can maximize concurrency on the server side. You can use this pattern to schedule work to be completed as capacity allows.|
|[Backends for Frontends](/azure/architecture/patterns/backends-for-frontends)|Individualizes the service layer of a workload by creating separate services that are exclusive to a specific frontend interface. This separation enables you to optimize in ways that might not be possible with a shared service layer. When you treat individual clients differently, you can optimize performance for a specific client's constraints and functionality.|
|[Bulkhead](/azure/architecture/patterns/bulkhead)|Introduces segmentation between components to isolate the blast radius of malfunctions. This design enables each bulkhead to be individually scalable to meet the needs of the task that's encapsulated in the bulkhead.|
|[Cache-Aside](/azure/architecture/patterns/cache-aside)|Optimizes access to frequently read data by introducing a cache that's populated on demand. The cache is then used on subsequent requests for the same data. This pattern is especially useful with read-heavy data that doesn't change often and can tolerate a certain amount of staleness. The goal of this implementation is to provide better performance in the system overall by offloading this type of data to a cache instead of sourcing it from its data store.|
|[Choreography](/azure/architecture/patterns/choreography)|Coordinates the behavior of autonomous distributed components in a workload by using decentralized, event-driven communication. This pattern can provide an alternative when performance bottlenecks occur in a centralized orchestration topology.|
|[Circuit Breaker](/azure/architecture/patterns/circuit-breaker)|Prevents continuous requests to a malfunctioning or unavailable dependency. A retry-on-error approach can lead to excessive resource utilization during dependency recovery and can also overload performance on a dependency that's attempting recovery.|
|[Claim Check](/azure/architecture/patterns/claim-check)|Separates data from the messaging flow, providing a way to separately retrieve the data related to a message. This pattern improves the efficiency and performance of message publishers, subscribers, and the message bus itself when the system handles large data payloads. It works by decreasing the size of messages and ensuring that consumers retrieve payload data only if necessary and at an opportune time.|
|[Competing Consumers](/azure/architecture/patterns/competing-consumers)|Applies distributed and concurrent processing to efficiently handle items in a queue. This model supports distributing load across all consumer nodes and dynamic scaling that's based on queue depth.|
|[Compute Resource Consolidation](/azure/architecture/patterns/compute-resource-consolidation)|Optimizes and consolidates compute resources by increasing density. This pattern combines multiple applications or components of a workload on a shared infrastructure. This conslolidation maximizes the utilization of computing resources by using spare node capacity to reduce overprovisioning. Container orchestrators are a common example. Large (vertically scaled) compute instances are often used in the resource pool for these infrastructures.|
|[Command and Query Responsibility Segregation (CQRS)](/azure/architecture/patterns/cqrs)|Separates the read and write operations of an application's data model. This separation enables targeted performance and scaling optimizations for each operation's specific purpose. This design is most helpful in applications that have a high read-to-write ratio.|
|[Deployment Stamps](/azure/architecture/patterns/deployment-stamp)|Provides an approach for releasing a specific version of an application and its infrastructure as a controlled unit of deployment, based on the assumption that the same or different versions will be deployed concurrently. This pattern often aligns to the defined scale units in your workload: as additional capacity is needed beyond what a single scale unit provides, an additional deployment stamp is deployed for scaling out.|
|[Event Sourcing](/azure/architecture/patterns/event-sourcing)|Treats state change as series of events, capturing them in an immutable, append-only log. Depending on your workload, this pattern, usually combined with CQRS, an appropriate domain design, and strategic snapshotting, can improve performance. Performance improvements are due to the atomic append-only operations and the avoidance of database locking for writes and reads.|
|[Federated Identity](/azure/architecture/patterns/federated-identity)|Delegates trust to an identity provider that's external to the workload for managing users and providing authentication for your application. When you offload user management and authentication, you can spend application resources on other priorities.|
|[Gatekeeper](/azure/architecture/patterns/gatekeeper)|Offloads request processing that's specifically for security and access control enforcement before and after forwarding the request to a backend node. This pattern is often used to implement throttling at a gateway level rather than implementing rate checks at the node level. Coordinating rate state among all nodes isn't inherently performant.|
|[Gateway Aggregation](/azure/architecture/patterns/gateway-aggregation)|Simplifies client interactions with your workload by aggregating calls to multiple backend services in a single request. This design can incur less latency than a design in which the client establishes multiple connections. Caching is also common in aggregation implementations because it minimizes calls to backend systems.|
|[Gateway Offloading](/azure/architecture/patterns/gateway-offloading)|Offloads request processing to a gateway device before and after forwarding the request to a backend node. Adding an offloading gateway to the request process enables you to use less resources per-node because functionality is centralized at the gateway. You can optimize the implementation of the offloaded functionality independently of the application code. Offloaded platform-provided functionality is already likely to be highly performant.|
|[Gateway Routing](/azure/architecture/patterns/gateway-routing)|Routes incoming network requests to various backend systems based on request intents, business logic, and backend availability. Gateway routing enables you to distribute traffic across nodes in your system to balance load.|
|[Geode](/azure/architecture/patterns/geodes)|Deploys systems that operate in active-active availability modes across multiple geographies. This pattern uses data replication to support the ideal that any client can connect to any geographical instance. You can use it to serve your application from a region that's closest to your distributed user base. Doing so reduces latency by eliminating long-distance traffic and because you share infrastructure only with other uses that are currently using the same geode.|
|[Health Endpoint Monitoring](/azure/architecture/patterns/health-endpoint-monitoring)|Provides a way to monitor the health or status of a system by exposing an endpoint that's specifically designed for that purpose. You can use these endpoints to improve load balancing by routing traffic to only nodes that are reporting healthy and, in advanced models, even reporting available node capacity to take on work.|
|[Index Table](/azure/architecture/patterns/index-table)|This pattern optimizes data retrieval in distributed data stores by offering clients the ability to look up metadata so that data can be directly retrieved instead of doing full data store scans. Clients are then way pointed to their shard, partition, or endpoint; enabling potentially dynamic data partitioning to optimize performance.|
|Materialized View|This pattern uses precomputed views of data to optimize data retrieval. The materialized views store results of complex computations or queries without needing the database engine or client to recompute for every request, which reduces overall resource consumption.|
|[Priority Queue](/azure/architecture/patterns/priority-queue)|This pattern ensures that higher-priority items are processed and completed before lower-priority items. Having the ability to separate items based on business priority allows you to focus  performance efforts on the work that is most time sensitive.|
|[Publisher/Subscriber](/azure/architecture/patterns/publisher-subscriber)|This pattern seeks to decouple components in an architecture by replacing direct client-to-service or client-to-services communication with an intermediate message broker or event bus. The decoupling of publisher from consumers allows you to optimize the compute and code per consumer for the task it needs to perform for this message.|
|[Queue-Based Load Leveling](/azure/architecture/patterns/queue-based-load-leveling)|This pattern seeks to smooth out incoming requests or tasks by buffering them in a queue and letting the queue processor handle them at a controlled pace. This approach allows very intentional design on throughput performance as intake of requests do not need to correlate to the rate in which those same requests are processed.|
|[Scheduler Agent Supervisor](/azure/architecture/patterns/scheduler-agent-supervisor)|This pattern seeks to efficiently distribute and re-distribute tasks across a system based on observable factors available in the system. This pattern uses performance and capacity metrics to detect current utilization and route tasks to an agent with capacity. It also can be used to prioritize the execution of higher priority work over lower priority work.|
|[Sharding](/azure/architecture/patterns/sharding)|This pattern seeks to take load and direct it to a specific logical destination to handle the specific request, allowing optimizations to be made with data co-location to optimize the request. Using sharding in your scaling strategy, means that because this data or processing is isolated to a shard, it will only compete for resources for other requests coming to this shard. Shards can also be used to optimize around geography.|
|[Sidecar](/azure/architecture/patterns/sidecar)|This pattern is a method to extend the functionality of an application by encapsulating non-primary or cross-cutting concerns into a companion process that exists alongside of the main application. This pattern is a one way to extract cross-cutting concerns into a single process that is able to scale across multiple instances of the main process, reducing the need for deploying duplicate functionality for each instance of the application.|
|[Static Content Hosting](/azure/architecture/patterns/static-content-hosting)|This pattern seeks to optimize delivery of static content to workload clients by using a hosting platform designed with this purpose in mind. Offloading responsibility to an externalized host helps mitigate congestion, leaving your application platform focused on delivering on your business logic.|
|[Throttling](/azure/architecture/patterns/throttling)|This pattern put limits on the rate or throughput of incoming requests to resource or component. When the system is under high demand, this pattern helps mitigate congestion which can lead to performance bottlenecks. It can also be a proactive approach to avoiding noisy neighbor situations.|
|[Valet Key](/azure/architecture/patterns/valet-key)|This pattern grants security-restricted access to a resource without involving an intermediary resource to proxy the access. This offloads processing to be an exclusive relationship between the client and the resource without adding an ambassador component that needs to performantly handle all client requests. This benefit is most prominent when the proxy would not be adding additional value to the transaction.|

## Next steps

Review the cloud design patterns that support the other Azure Well-Architected Framework pillars:

- [Cloud design patterns that support reliability](../reliability/design-patterns.md)
- [Cloud design patterns that support security](../security/design-patterns.md)
- [Cloud design patterns that support operational excellence](../operational-excellence/design-patterns.md)
- [Cloud design patterns that support cost optimization](../cost-optimization/design-patterns.md)
