---
title: Cloud design patterns that support reliability
description: Learn about industry patterns that can help you address common challenges in cloud workloads and achieve operational excellence.  
author: ckittel
ms.author: chkittel
ms.date: 11/15/2023
ms.topic: conceptual
---

# Cloud design patterns that support reliability

Workload architecture should be designed using existing industry patterns that address common challenges faced by workloads. Patterns are a powerful tool to help make intentional tradeoffs within a workload, optimizing for desired outcome. These patterns represent real-world experience, are designed with cloud scale & operating models in mind, and are inherently vendor-agnostic. Using well-known patterns as part of standardizing your workload design itself is a component of operational excellence.

Many design patterns directly support one or more architecture pillars, often introducing tradeoffs with their implementation. 

Design patterns that support the reliability pillar are ones that prioritize concepts like workload availability, self-preservation, recovery, data & processing integrity, and malfunction containment.

The following table summarizes cloud design patterns that support the goals of reliability.

|Pattern|Summary|
|-|-|
|[Ambassador](/azure/architecture/patterns/ambassador)|This pattern seeks to encapsulate and manage network communications by offloading diverse cross-cutting concerns related to network communication details, ultimately initiating communication on behalf of its client. This mediation point provides an affordance to add reliability patterns to network communication, such as retry or buffering.|
|[Backends for Frontends]()|This pattern individualizes the service layer of a workload by creating separate services that are exclusive to a specific front-end interface. This separation of concerns means that a malfunction in the service layer supporting one client might not impact the availability of another client's access. When treating different clients differently, you can prioritize reliability efforts based on expected client access patterns.|
|[Bulkhead]()|This pattern introduces intentional and complete segmentation between components to isolate the blast radius of malfunction. This failure isolation strategy strives to contain faults to just the bulkhead with the issue, preventing impact to other bulkheads.|
|[Cache-Aside]()|This pattern optimizes access to frequently read data by introducing a cache that is populated on-demand, which is then used on subsequent requests for the same data. Caching creates data replication, and in limited ways, can be used to preserve the availability of frequently accessed data while the origin data store might be temporarily unavailable. Likewise, in this pattern, a malfunction in the cache can enable the workload to still operate by falling back to the origin data store.|
|[Circuit Breaker]()|This pattern prevents continuous requests to a malfunctioning or unavailable dependency. This prevents overloading a faulting dependency and also can be used to trigger graceful degradation within the workload. Circuit breakers often are coupled with automatic recovery as well, and in this way work for both self-preservation and self-healing.|
|[Claim Check]()|This pattern separates data from the messaging flow, providing a reference to retrieve the data related to the message instead of bundling the data with the message. Message busses do not have a lot of reliability and disaster recovery affordances often found in dedicated data stores, so externalizing the data from the message can provide increased reliability of the underlying data and be helpful as a message queue recovery technique in a disaster.|
|[Compensating Transaction]()|This pattern focuses on recovering from failures by intentionally designing a mechanism to reverse the effects of previously applied actions. This pattern addresses malfunction in critical workload paths through purpose built compensation actions, which may involve processes like directly rolling back data changes, breaking transaction locks, or even executing native system behavior to reverse the effect.|
|[Competing Consumers]()|This pattern applies distributed and concurrent processing to efficiently handle items in a queue. This model builds redundancy in queue processing by treating consumers as replicas, where a instance failure does not prevent other consumers from processing queue messages.|
|[Event Sourcing]()|This pattern treats state change as series of events, capturing them in an immutable, append-only log. This pattern can used when an accurate and reliable history of changes is crucial in a complex business process. It also help facilitate state reconstruction if state stores need to be recovered.|
|[Federated Identity]()|This pattern delegates trust to an identity provider that is external to the workload to manage users and to provide the authentication for your application. Offloading user management and authentication shifts reliability for those components to the identity provider, which usually has a very high SLA. Also in workload disaster recovery situation, authentication components likely do not need to be addressed as part of the workload's recovery plan.|
|[Gateway Aggregation]()|This pattern simplifies client interactions with your workload by shifting the calling multiple backend services and coordinating their results as a single response to the client. This topology allows you to shift transient fault handling from being a distributed implementation across clients to a centralized implementation.|
|[Gateway Offloading]()|This pattern is used to offload request processing in a gateway device before and after forwarding the request to a backend node. Offloading responsibility to a gateway reduces the complexity of application code on backend nodes, and in some cases completely replaces that functionality with a reliable platform-provided feature.|
|[Gateway Routing]()|This pattern is used to route incoming network requests to various backend systems based on request intents, business logic, and backend availability. Having gateway routing allows you to route traffic to only healthy nodes in your system.|
|[Geode]()|This pattern describes system that operate in active-active availability modes across multiple geographies. This pattern uses data replication to support the ideal that any client could connect to any geo instance. This pattern can be used to help survive one or more regional outages.|
|[Health Endpoint Monitoring]()|This pattern is a technique to monitor the health or status of a system by exposing an endpoint specifically designed to provide information about the system's health. These endpoints can be used to help drive your workload's health model, alerting, and dashboarding. They can also be used as signals for self-healing remediation approaches.|
|[Index Table]()|This pattern optimizes data retrieval in distributed data stores by offering clients the ability to look up metadata so that data can be directly retrieved instead of doing full data store scans. Because clients are way pointed to their shard, partition, or endpoint through a lookup process; this pattern can be used to facilitate a failover approach for data access.|
|[Leader Election]()|This pattern establishes a coordinator over distributed nodes taking on responsibilities related to accomplishing a goal. The pattern seeks to avoid impact from node malfunction by reliably redirecting work and also implements failover through consensus algorithms when faced with leader malfunction.|
|[Pipes and Filters]()|This pattern breaks down complex data processing into a series of independent stages connected to each other to achieve a specific outcome. The single responsibility principle of each stage allows focused reliability attention without the distraction of a comingled, monolithic data processing implementation.|
|[Priority Queue]()|This pattern ensures that higher-priority items are processed and completed before lower-priority items. Having the ability to separate items based on business priority allows you to focus reliability efforts to the most critical work.|
|[Publisher/Subscriber]()|This pattern seeks to decouple components in an architecture by replacing direct client-to-service or client-to-services communication with an intermediate message broker or event bus.|
|[Queue-Based Load Leveling]()|This pattern seeks to smooth out incoming requests or tasks by buffering them in a queue and letting the queue processor handle them at a controlled pace. This approach can provide resilience against sudden spikes in demand by decoupling the arrival of tasks from their processing. It also supports isolating a malfunction in queue processing from impacting intake. It  supports isolating a malfunction in queue processing from impacting intake.|
|[Rate Limiting]()|This pattern seeks to control the rate at which a client makes requests to reduce throttling errors and avoid unbounded retry-on-error situations. This self-preservation tactic is applied to protect the client, by acknowledging limitations and costs of communicating to a service such that the client code is designed to avoid hitting those limits. This is done through deliberate control of number and/or size of operations sent to the service over a specific time period.|
|[Retry]()|This pattern seeks ways to address failures that might be indicative of a transient/intermittent fault by retrying select operations, in a controlled way. Mitigating transient faults in a distributed system is a key technique to enhance a workload's resilience.|
|[Saga distributed transactions]()|This pattern seeks to coordinate long-running and potentially complex transactions by decomposing the work into sequences of smaller, independent transactions. Each transaction must also have coded compensating actions to reverse failures in execution and maintain integrity. Since monolithic transactions across multiple distributed systems, in most cases, are not possible, this pattern provides consistency and reliability through atomicity and compensation.|
|[Scheduler Agent Supervisor]()|This pattern seeks to efficiently distribute and re-distribute tasks across a system based on observable factors available in the system. This pattern uses health metrics to detect failures and reroute tasks to a healthy agent to mitigate the impact from malfunction.|
|[Sequential Convoy]()|This pattern seeks to maintain concurrent messaging ingress while also supporting in-order processing execution. A correct implementation can eliminate hard to troubleshoot race conditions, contentious message handling, or other workarounds to address out-of-order messages that can lead to malfunction.|
|[Sharding]()|This pattern seeks to take load and direct it to a specific logical destination to handle the specific request, allowing optimizations to be made with data co-location to optimize the request. Because this data or processing is isolated to the shard, a malfunction one shard remains isolated to only that shard. |
|[Strangler Fig]()|This pattern is a planful approach to systematically replacing components of a running system with a new implementation, often in conjunction with migrating or modernizing a legacy system. The incremental approach offered by this pattern is a risk mitigation strategy especially relevant during a potentially risky transition.|
|[Throttling]()|This pattern put limits on the rate or throughput of incoming requests to resource or component. The limits can be designed to help prevent resource exhaustion that might lead to malfunction. It also can be used as a control mechanism in a graceful degradation plan.|

## Next steps

See what cloud design patterns support the other pillars of architecture.

- [Cloud design patterns that support security]()
- [Cloud design patterns that support cost optimization]()
- [Cloud design patterns that support operational excellence]()
- [Cloud design patterns that support performance efficiency]()
