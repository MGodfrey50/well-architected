---
title: Azure Well-Architected Framework review - Log Analytics
description: Provides architectural best practices according to Azure Well-Architected Framework for Log Analytics workspaces in Azure Monitor. 
ms.topic: conceptual
author: bwren
ms.author: bwren
ms.date: 12/14/2023
ms.reviewer: bwren
---

# Azure Well-Architected Framework perspective on Log Analytics

In order to maintain workload functionality and performance, they need to be monitored in diverse ways and for diverse reasons. Log Analytics workspaces are Azure's primary log and metric sink for a significant portion of that monitoring data. Workspaces support multiple features in Azure Monitor including ad-hoc queries, visualizations, and alerts. Start with [Monitoring and diagnostics guidance](/azure/architecture/best-practices/monitoring) which presents general monitoring principles and identifies the different types of data and required analysis supported by Azure Monitor and enabled by data stored in the workspace.

This article assumes that you understand system design principles and have a working knowledge of Log Analytics workspaces and features in Azure Monitor that populate it with operational workload data. For more information, see [Log Analytics workspace overview](/azure/azure-monitor/logs/log-analytics-workspace-overview).

> [!IMPORTANT]
>
> **How to use this guide**
>
> Each section has a _design checklist_ that presents architectural areas of concern along with design strategies localized to the technology scope. 
>
> Also included are _recommendations_ on the technology capabilities or deployment topologies that can help materialize those strategies. The recommendations don't represent an exhaustive list of all configurations available for Log Analytics workspaces and its related Azure Monitor resources. Instead, they list the key recommendations mapped to the design perspectives. Use the recommendations to build your proof-of-concept, design your workload monitoring environment, or optimize your existing workload monitoring solution.

##### Technology scope

This review focuses on the interrelated decisions for these Azure resources.
- Log Analytics workspaces
- Workload operational log data
- Diagnostic settings on Azure resources in your workload
  
## Reliability

The purpose of the Reliability pillar is to provide continued functionality by **building enough resilience and the ability to recover fast from failures**.

The [**Reliability design principles**](/azure/well-architected/resiliency/principles) provide a high-level design strategy applied for individual components, system flows, and the system as a whole.


The reliability situations to consider for Log Analytics workspaces are availability of the workspace and protection of collected data in the rare case of failure of an Azure datacenter or region. There's currently no standard feature for failover between workspaces in different regions, but there are strategies that you can use if you have particular requirements for availability or compliance.

### Design checklist

Start your design strategy based on the [**design review checklist for Reliability**](../reliability/checklist.md) and determine its relevance to your business requirements while keeping in mind the SKUs and features of VMs and their dependencies. Extend the strategy to include more approaches as needed.

> [!div class="checklist"]
> - **Review [service limits for Log Analytics workspaces](/azure/azure-monitor/service-limits#log-analytics-workspaces)** to understand restrictions on data collection and retention, and other aspects of the service. These limits will help you determine how to properly design your workload observability strategy. Be sure to review [Azure Monitor service limits](/azure/azure-monitor/service-limits) as well as many of the functions discussed therein work hand-in-hand with Log Analytics workspaces, like queries.
> - **Plan for workspace resilience and recovery.** Log Analytics workspaces are regional, with no built-in support for cross-regional redundancy or replication. Also, availability zone redundancy options are limited. As such, you should determine the reliability requirements of your workspaces and strategize to meet those targets. Your requirements may stipulate that your workspace must be resilient to datacenter failures or regional failures, or they may stipulate that you must be able to recover your data to a new workspace in a failover region. Each of these scenarios require additional resources and processes to be put in place to be successful, so balancing your reliability targets with cost and complexity should be carefully considered.
> - **Choose the right deployment regions to meet your reliability requirements.** Deploy your Log Analytics workspace and data collection endpoints (DCEs) co-located with the workload components emitting operational data. Your choice of the appropriate region to deploy your workspace and your DCEs in should be informed by your decision-making about where to [deploy your workload](/azure/cloud-adoption-framework/ready/azure-setup-guide/regions). You may need to weigh the regional availability of certain Log Analytics functionality, like dedicated clusters, against other factors more central to your workload’s reliability, cost, and performance requirements.
> - **Ensure that your observability systems themselves are healthy.** Like any other component of your workload, you need to be able to ensure that your monitoring and logging systems are functioning properly. To accomplish this, enable features that send health data signals to your operations teams specifically about your Log Analytics workspaces and associated resources.
 
### Recommendations

| Recommendation | Benefit |
|:---|:---|
| Do not include your Log Analytics workspaces in your [workload's critical path](/azure/well-architected/reliability/identify-flows). Your workspaces are important to a functioning observability system, but the functionality of your workload should not depend on them. | Keeping your workspaces and associated functions out of your workload’s critical path minimizes the risk of issues affecting your observability system from affecting the runtime execution of your workload. |
| To support high durability of workspace data, Log Analytics workspaces should be deployed into a [region](/azure/azure-monitor/logs/availability-zones#data-resilience---supported-regions) that supports data resilience. Data resilience is only possible through linking of the workspace to a [dedicated cluster](/azure/azure-monitor/logs/logs-dedicated-clusters) in the same region. | Using a dedicated cluster allows you to spread the associated workspaces across availability zones, which offers protection against datacenter outages. If you don't collect enough data now to justify a dedicated cluster, this preemptive regional choice will support future growth. |
| Choose your workspace deployment based on proximity to your workload.<br><br> Use data collection endpoints (DCE) in the same region as the Log Analytics workspace. | Deploy your workspace in the same region as the instances of your workload. Having your workspace and DCEs in the same region as your workload mitigates the risk of being impacted by outages in other regions.<br><br> DCEs are used by the Azure Monitor agent and the Logs ingestion API to send workload operational data to a Log Analytics workspace. You may need multiple DCEs even though your deployment only has a single workspace. See [How to set up data collection endpoints based on your deployment](/azure/azure-monitor/essentials/data-collection-endpoint-overview#how-to-set-up-data-collection-endpoints-based-on-your-deployment) for details on how to configure DCEs for your particular environment.<br<br> If your workload is deployed in an active-active design, consider the use of multiple workspaces and DCEs spread across the regions your workload is deployed in.<br><br> Deploying workspaces in multiple regions will add complexity to your environment. Balance the criteria detailed in [Design a Log Analytics workspace architecture](/azure/azure-monitor/logs/workspace-design) with your availability requirements. |
| If you require the **workspace to be available** in the case of a region failure, or you don't collect enough data for a dedicated cluster, configure data collection to send critical data to multiple workspaces in different regions, also known as log multicasting.<br><br>For example, configure DCRs for multiple workspaces for Azure Monitor agent running on virtual machines, and multiple diagnostic settings to collect resource logs from Azure resources, configured to send the logs to multiples workspace. | In this way workload operational data will be available in the alternate workspace in case of regional failure. However, be aware that resources that rely on the data such as alerts and workbooks wouldn't automatically be replicated to the other regions. Consider storing ARM templates for critical alerting resources with configuration for the alternate workspace or deploying them in all regions but disabling them to prevent redundant alerts. Both options support quickly enablement in a regional failure.<br><br>Tradeoff: This configuration results in duplicate ingestion and retention charges so only use it for critical data. |
| If you require **data to be protected** in the case of datacenter or region failure, configure [data export](/azure/azure-monitor/logs/logs-data-export) from the workspace to save data in an alternate location.<br><br>This option is similar to the previous option of multicasting the data to different workspaces, but has a lower cost because the extra data is written to storage.<br><br>Use [Azure Storage redundancy options](/azure/storage/common/storage-redundancy#redundancy-in-a-secondary-region), including GRS and GZRS, to further replicate this data to other regions.<br><br>Data export doesn't provide resiliency against incidents impacting the regional ingestion pipeline. | While the historic operational log data may not be readily quarriable in the exported state, but it ensures the data will survive a prolonged regional outage and can be accessed and retained for extended period.<br><br>If you require export of [tables that aren't supported by data export](/azure/azure-monitor/logs/logs-data-export#limitations), you can use other methods of exporting data, including Logic apps, to protect your data.<br><br>In order for this strategy to work as a viable recovery plan, you must plan to have processes in place to reconfigure diagnostic settings for your resources in Azure as well as on all agents that provide telemetry, and to manually rehydrate your exported data into a new workspace. As with the above option, you will also need to define processes for those resources that rely on the data like alerts and workbooks. |
| For mission-critical workloads requiring high availability, consider implementing a federated workspace model that uses multiple workspaces to provide high availability in the case of regional failure. | [Mission-critical](/azure/well-architected/mission-critical/mission-critical-overview) provides prescriptive best practice guidance for designing highly reliable applications on Azure. The design methodology includes a federated workspace model with multiple Log Analytics workspaces to deliver [high availability](/azure/well-architected/mission-critical/mission-critical-design-methodology#select-a-reliability-tier) in the case of multiple failures, including the failure of an Azure region.<br><br> This strategy eliminates egress costs across regions and remains operational with a region failure, but it requires additional complexity that you must manage with configuration and processes described in [Health modeling and observability of mission-critical workloads on Azure](/azure/well-architected/mission-critical/mission-critical-health-modeling).|
| Use Infrastructure-as-code (IaC) to deploy and manage your workspaces and associated functions. | Automating as much of your deployment and your mechanisms for resilience and recovery as practical will ensure that these operations are reliable. You will save critical time in your operations processes and minimize the risk of human error.<br><br> Ensure that functions like saved log queries are also defined through your IaC to recover them to a new region if recovery is required. |
| Design DCRs with a single responsibility principle to keep DCR rules simple.<br><br>While one DCR could be loaded with all the input, rules, and destinations for the source systems, it's preferable to design narrowly focused rules that rely on fewer data sources. Use composition of rule assignments to arrive at the desired observability scope for the logical target.<br><br> Additionally, minimize transformation in DCRs | Using narrowly focused DCRs minimizes the risk of a misconfiguration of a rule having broad impact by limiting impact only to the scope for which the DCR was built. See [Best practices for data collection rule creation and management in Azure Monitor](/azure/azure-monitor/essentials/data-collection-rule-best-practices).<br><br> While transformation can be powerful and necessary in some situations, it can be challenging to test and troubleshoot the KQL work being done. When possible, minimize the risk of data loss by ingesting the data raw and handling transformations downstream at query time.  |
| When setting a [daily cap](/azure/azure-monitor/logs/daily-cap) or a retention policy, ensure that you are maintaining your reliability requirements by ingesting and retaining the logs that you need. | A daily cap stops the collection of data for a workspace once a given amount has been reached, which helps you maintain control over your ingestion volume. However, using this feature should only be done after careful planning. Ensure that your daily cap is not being hit with regularity. If this scenario happens, it means that your cap is set too restrictively and needs to be reconfigured, so that you don’t miss critical signals coming from your workload.<br><br> Likewise, any lowering of your data retention policy should be carefully and thoughtfully approached to ensure that you do not inadvertently lose critical data. |
| Use [Log Analytics workspace insights](/azure/azure-monitor/logs/workspace-design) to track ingestion volume, ingested data vs your data cap, unresponsive log sources, and failed queries among other data. Create [health status alert](/azure/azure-monitor/logs/log-analytics-workspace-health#view-log-analytics-workspace-health-and-set-up-health-status-alerts) to proactively notify you if a workspace becomes unavailable because of a datacenter or regional failure. | This strategy will ensure that you are able to successfully monitor the health of your workspaces and proactively act if the health is at risk of degrading. Like any other component of your workload, it is critical that you are aware of health metrics and can identify trends to improve your reliability over time.

### Azure Policy

Azure offers no policies related to reliability of Log Analytics workspaces. [Custom policies](/azure/governance/policy/tutorials/create-custom-policy-definition) can be created to build compliance guardrails around your workspace deployments, such as ensuring workspaces are associated to a dedicated cluster.

While not directly related to the reliability of Log Analytics workspaces themselves, there are Azure policies for nearly every service available to ensure that diagnostics settings are enabled for that service; validating that the service's log data is flowing into a Log Analytics workspace. All services in workload architecture should be sending their log data to a Log Analytics workspace for their own reliability needs, and these policies can help enforce this. Likewise, policies exist to ensure agent-based platforms, such as virtual machines and Kubernetes, have the agent installed.

### Azure Advisor

Azure offers no Azure Advisor recommendations related to the reliability of Log Analytics workspaces.


## Security

The purpose of the Security pillar is to provide **confidentiality, integrity, and availability** guarantees to the workload.

The [**Security design principles**](/azure/well-architected/security/security-principles) provide a high-level design strategy for achieving those goals by applying approaches to the technical design around your monitoring and logging solution.

### Design checklist

Start your design strategy based on the [**design review checklist for Security**](../security/checklist.md) and identify vulnerabilities and controls to improve the security posture. Extend the strategy to include more approaches as needed.

> [!div class="checklist"]
> - **Review the Azure Monitor [security baseline](/security/benchmark/azure/baselines/azure-monitor-security-baseline)** and [Manage access to Log Analytics workspaces](/azure/azure-monitor/logs/manage-access) articles for guidance on security best practices. 
> - **Deploy your workspaces with segmentation as a cornerstone principle.** Segmentation should be implemented at the networking, data, and access levels to ensure that your workspaces are isolated to the appropriate degree and are protected from unauthorized access to the highest degree possible, while still meeting your business requirements for reliability, cost optimization, operational excellence, and performance efficiency.
> - **Ensure that you can audit workspace reads and writes activities and associated identities.** Attackers can benefit from viewing operational logs and a compromised identity can lead to log injection attacks. Enable auditing of operations run from the Azure Portal or through API interactions, and the associated users. If you are not set up to audit your workspace, you might be putting your organization at risk of being in breach of compliance requirements.
> - **Implement robust network controls.** Secure your network access to your workspace and your logs through network isolation and firewall functions. Insufficiently configured network controls may put you at risk of being accessed by unauthorized or malicious actors.
> - **Determine what types of data need immutability or long-term retention.** Your log data should be treated with the same rigor as workload data inside production systems. Include log data in your data classification practices to ensure that you are successfully storing sensitive log data according to its compliance requirements.
> - **Protect log data at rest through encryption.** Segmentation alone will not completely protect confidentiality of your log data. If unauthorized raw access is achieved, having the log data encrypted at rest will prevent bad actors from using that data outside of your workspace.
> - **Protect sensitive log data through obfuscation.** Just like workload data residing in production systems, you must take extra measures to ensure confidentiality is retained for sensitive information that may be intentionally or unintentionally present in operational logs. Using obfuscation methods will help you hide sensitive log data from unauthorized eyes.

### Recommendations

| Recommendation | Benefit |
|:---|:---|
| (Control plane) Secure your workspace through private networking and segmentation measures.<br><br>Use [private link](/azure/azure-monitor/logs/private-link-security) functionality to limit communications between log sources and your workspaces to private networking. |Using private link also allows you control which virtual networks can access a given workspace, further bolstering your security through segmentation. |
| (Control Plane) Use customer managed keys if you require your own encryption key to protect data and saved queries in your workspaces. | Azure Monitor ensures that all data and saved queries are encrypted at rest using Microsoft-managed keys (MMK). If you require your own encryption key and collect enough data for a [dedicated cluster](/azure/azure-monitor/logs/logs-dedicated-clusters), use [customer-managed key](/azure/azure-monitor/logs/customer-managed-keys) for greater flexibility and key lifecycle control. You can encrypt data using your own key in [Azure Key Vault](/azure/key-vault/general/overview), for control over the key lifecycle, and ability to revoke access to your data.<br><br> If you use Microsoft Sentinel, then make sure that you're familiar with the considerations at [Set up Microsoft Sentinel customer-managed key](/azure/sentinel/customer-managed-keys#considerations).  |
| (Control Plane) Configure [Log query auditing](/azure/azure-monitor/logs/query-audit) to track which users are running queries.<br><br>Configure the audit logs for each workspace to be sent to the local workspace or consolidate in a dedicated security workspace if you separate your operational and security data. Use [Log Analytics workspace insights](/azure/azure-monitor/logs/log-analytics-workspace-insights-overview) to periodically review this data and consider creating log query alert rules to proactively notify you if unauthorized users are attempting to run queries. | Log query auditing records the details for each query that's run in a workspace. Treat this audit data as security data and secure the [LAQueryLogs](/azure/azure-monitor/reference/tables/laquerylogs) table appropriately.  This strategy will bolster your security posture by helping ensure that unauthorized access is caught immediately if it ever happens. |
| (Control Plane) Use Microsoft Entra ID instead of API keys for workspace API access where available. | [API key-based access](/azure/azure-monitor/logs/api/overview#api-key-authentication-for-sample-data) to the Query APIs does not leave a per-client audit trail. Use sufficiently scoped [Entra ID-based access](/azure/azure-monitor/logs/api/overview#microsoft-entra-authentication-for-workspace-data) so that programmatic access can be properly audited. |
| (Data) Configure access for different types of data in the workspace required for different roles in your organization.<br><br>Set the [access control mode](/azure/azure-monitor/logs/manage-access#access-control-mode) for the workspace to *Use resource or workspace permissions* to allow resource owners to use [resource-context](/azure/azure-monitor/logs/manage-access#access-mode) to access their data without being granted explicit access to the workspace.<br><br>Use [table level RBAC](/azure/azure-monitor/logs/manage-access#set-table-level-read-access) for users who require access to a set of tables across multiple resources.  | This simplifies your workspace configuration and helps to ensure users will not be able to access operational data they shouldn't.<br><br>Assign the appropriate [built-in role](/azure/azure-monitor/logs/manage-access#azure-rbac) to grant workspace permissions to administrators at either the subscription, resource group, or workspace level depending on their scope of responsibilities.<br><br>Users with table permissions have access to all the data in the table regardless of their resource permissions.<br><br>See [Manage access to Log Analytics workspaces](/azure/azure-monitor/logs/manage-access) for details on the different options for granting access to data in the workspace. |
| (Data) Export log that requires long term retention or immutability.<br><br>Use [data export](/azure/azure-monitor/logs/logs-data-export) to send data to an Azure Storage account with [immutability policies](/azure/storage/blobs/immutable-policy-configure-version-scope) to protect against data tampering. Not every type of log has the same relevance for compliance, auditing, or security, so determine the specific data types that should be exported. | You may have collected audit data in your workspace that's subject to regulations requiring its long-term retention. Data in a Log Analytics workspace can't be altered, but it can be [purged](/azure/azure-monitor/logs/personal-data-mgmt#exporting-and-deleting-personal-data). Exporting a copy of the operational data for retention purposes allows you to build a solution that meets your compliance requirements.  |
| (Data) Implement [Double encryption](/azure/storage/common/storage-service-encryption#doubly-encrypt-data-with-infrastructure-encryption) for the workspace.<br><br>This feature requires a [dedicated cluster](/azure/azure-monitor/logs/logs-dedicated-clusters) which has a daily minimum ingestion limit. | Double encryption provides an extra layer of encryption to protect against a scenario where one of the encryption algorithms or keys may be compromised.  |
| (Data) Determine a strategy to filter or obfuscate sensitive data in your workspace.<br><br>You may be collecting data that includes [sensitive information](/azure/azure-monitor/logs/personal-data-mgmt). Filter records that shouldn't be collected using the configuration for the particular data source. Use a [transformation](/azure/azure-monitor/essentials/data-collection-transformations) if only particular columns in the data should be removed or obfuscated.<br><br>If you have standards that require the original data to be unmodified, then you can use the ['h' literal](/azure/data-explorer/kusto/query/scalar-data-types/string#obfuscated-string-literals) in KQL queries to obfuscate query results displayed in workbooks. |  |

### Azure Policy

Azure offers some policies related to the security of Log Analytics workspaces to help enforce your desired security posture. Examples of such policies are:
- [Azure Monitor Logs clusters should be encrypted with customer-managed key](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2F1f68a601-6e6d-4e42-babf-3f643a047ea2)
- [Azure Monitor Logs clusters should be created with infrastructure-encryption enabled (double encryption)](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2Fea0dfaed-95fb-448c-934e-d6e713ce393d)
- [Saved-queries in Azure Monitor should be saved in customer storage account for logs encryption](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2Ffa298e57-9444-42ba-bf04-86e8470e32c7)
- [Log Analytics Workspaces should block non-Azure Active Directory based ingestion](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2F94c1f94d-33b0-4062-bd04-1cdc3e7eece2)

Azure also offers numerous policies to help enforce private link configuration, such as [Log Analytics workspaces should block log ingestion and querying from public networks](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2F6c53d030-cc64-46f0-906d-2bc061cd1334) or even configuring the solution through DINE policies such as [Configure Azure Monitor Private Link Scope to use private DNS zones](https://portal.azure.com/#blade/Microsoft_Azure_Policy/PolicyDetailBlade/definitionId/%2Fproviders%2FMicrosoft.Authorization%2FpolicyDefinitions%2F437914ee-c176-4fff-8986-7e05eb971365).

### Azure Advisor

Azure offers no Azure Advisor recommendations related to the security of Log Analytics workspaces.

## Cost optimization

Cost Optimization focuses on **detecting spend patterns, prioritizing investments in critical areas, and optimizing in others** to meet the organization's budget while meeting business requirements.

The [Cost Optimization design principles](../cost-optimization/principles.md) provide a high-level design strategy for achieving those goals and making tradeoffs as necessary in the technical design related to your monitoring and logging solution.

See [Azure Monitor Logs cost calculations and options](/azure/azure-monitor/cost-usage) to understand how data charges are calculated for your Log Analytics workspaces.


### Design checklist

Start your design strategy based on the [**design review checklist for Cost Optimization**](../cost-optimization/checklist.md) for investments and fine tune the design so that the workload is aligned with the budget allocated for the workload. Your design should use the right Azure capabilities, monitor investments, and find opportunities to optimize over time.

> [!div class="checklist"]
> - **Perform cost modeling exercises** to understand your current workspace costs and forecast your costs relative to workspace growth.  Analyze your growth trends in your workload and ensure that you understand plans for workload expansion to properly forecast your future opertional logging costs.
> - **Choose the right billing model.** Use your cost model to determine the best [billing model](/azure/azure-monitor/logs/cost-logs) for your scenario. How you use your workspaces currently, and how you plan to you use them as your workload evolves will determine whether a pay-as-you-go or a commitment tier model is the best fit for your scenario.<br><br> Remember that you can choose different billing models for each workspace, and you can combine workspace costs in certain cases, so you can be granular in your analysis and decision-making.
> - **Collect just the right amount of log data.** Perform regularly scheduled analysis of your diagnostic settings on your resources, data collection rule configuration, and custom application code logging to ensure that you aren't collecting unnecessary log data.
> - **Treat nonproduction environments differently than production.** Review your nonproduction environments to ensure that you have configured your diagnostic settings and retention policies appropriately. These can often be significantly less robust than production, especially for dev/test or sandbox environments.

### Recommendations

| Recommendation | Benefit |
|:---|:---|
| If you use Microsoft Sentinel to analyze security logs, consider using a separate workspace to store those logs. | Using a dedicated workspace for log data that will be used by your SIEM can help you control costs as those workspaces used by Sentinel are subject to [Sentinel pricing](/azure/sentinel/billing?tabs=simplified%2Ccommitment-tiers#how-youre-charged-for-microsoft-sentinel. Your security requirements will dictate the types of logs that are required to be included in your SIEM solution, but you may be able to exclude operational logs, which would be charged at the standard Log Analytics pricing if they're in a separate workspace. |
| Configure pricing tier for the amount of data that each Log Analytics workspace typically collects. | By default, Log Analytics workspaces will use pay-as-you-go pricing with no minimum data volume. If you collect enough data, you can significantly decrease your cost by using a [commitment tier](/azure/azure-monitor/logs/cost-logs#commitment-tiers), which allows you to commit to a daily minimum of data collected in exchange for a lower rate. If you collect enough data across workspaces in a single region, you can link them to a [dedicated cluster](/azure/azure-monitor/logs/logs-dedicated-clusters) and combine their collected volume using [cluster pricing](/azure/azure-monitor/logs/cost-logs#dedicated-clusters).<br><br>See [Azure Monitor Logs cost calculations and options](/azure/azure-monitor/logs/cost-logs) for details on commitment tiers and guidance on determining which is most appropriate for your level of usage. See [Usage and estimated costs](/azure/azure-monitor/usage-estimated-costs#usage-and-estimated-costs) to view estimated costs for your usage at different pricing tiers.  |
| Configure data retention and archiving. | There is a charge for retaining data in a Log Analytics workspace beyond the default of 31 days (90 days if Sentinel is enabled on the workspace and 90 days for Application insights data). Consider your particular requirements for having data readily available for log queries. You can significantly reduce your cost by configuring [Archived Logs](/azure/azure-monitor/logs/data-retention-archive), which allows you to retain data for up to seven years and still access it occasionally using [search jobs](/azure/azure-monitor/logs/search-jobs) or [restoring a set of data](/azure/azure-monitor/logs/restore) to the workspace. |
| Configure tables used for debugging, troubleshooting, and auditing as Basic Logs. | Tables in a Log Analytics workspace configured for [Basic Logs](/azure/azure-monitor/logs/basic-logs-configure) have a lower ingestion cost in exchange for limited features and a charge for log queries. If you query these tables infrequently and don't use them for alerting, this query cost can be more than offset by the reduced ingestion cost. |
| Limit data collection from data sources for the workspace. | The primary factor for the cost of Azure Monitor is the amount of data that you collect in your Log Analytics workspace, so you should ensure that you collect no more data that you require to assess the health and performance of your services and applications. Ensure that the diagnostic settings that you configure for each resource has the appropriate categories selected that will get you the amount of operational data that you need and use to successfully manage your workload, and not data that gets ignored.<br><br>Tradeoff: There may be a tradeoff between cost and your monitoring requirements. For example, you may be able to detect a performance issue more quickly with a high sample rate, but you may want a lower sample rate to save costs. Most environments will have multiple data sources with different types of collection, so you need to balance your particular requirements with your cost targets for each. See [Cost optimization in Azure Monitor](/azure/azure-monitor/best-practices-cost) for recommendations on configuring collection for different data sources. |
| Regularly analyze workspace usage data to identify trends and anomalies.<br><br>Use [Log Analytics workspace insights](/azure/azure-monitor/logs/log-analytics-workspace-insights-overview) to periodically review the amount of data collected in your workspace.  Further analyze data collection using methods in [Analyze usage in Log Analytics workspace](/azure/azure-monitor/logs/analyze-usage) to determine if there's additional configuration that can decrease your usage further. | In addition to helping you understand the amount of data collected by different sources, it will identify anomalies and upward trends in data collection that could result in excess cost. This is particularly important when you add a new set of data sources to your workload, such as a new set of virtual machines, enable new Azure diagnostics settings on a service, or change log levels in your application. |
| Create an alert when data collection is high. | To avoid unexpected bills, you should be [proactively notified anytime you experience excessive usage](/azure/azure-monitor/logs/analyze-usage#send-alert-when-data-collection-is-high). Notification allows you to address any potential anomalies before the end of your billing period. |
| Consider a daily cap as a preventative measure to ensure that you don't exceed a particular budget. | A [daily cap](/azure/azure-monitor/logs/daily-cap) disables data collection in a Log Analytics workspace for the rest of the day after your configured limit is reached. This shouldn't be used as a method to reduce costs as described in [When to use a daily cap](/azure/azure-monitor/logs/daily-cap#when-to-use-a-daily-cap), but instead to prevent runaway ingestion due to misconfiguration or abuse.<br><br>If you do set a daily cap, in addition to [creating an alert when the cap is reached](/azure/azure-monitor/logs/log-analytics-workspace-health#view-log-analytics-workspace-health-and-set-up-health-status-alerts), ensure that you also [create an alert rule to be notified when some percentage has been reached (90% for example)](/azure/azure-monitor/logs/analyze-usage#send-alert-when-data-collection-is-high). This gives you an opportunity to investigate and address the cause of the increased data before the cap shuts off critical data collection from your workload. |

### Azure Policy

Azure offers no policies related to cost optimization of Log Analytics workspaces. [Custom policies](/azure/governance/policy/tutorials/create-custom-policy-definition) can be created to build compliance guardrails around your workspace deployments, such as ensuring workspaces have desired retention settings.

### Azure Advisor

Azure Advisor will make recommendations to move specific tables in a workspace to the low-cost Basic log data plan for tables that receive relatively high ingestion volume. Understand the limitations using Basic logs before switching. See, [When should I use Basic logs?](/azure/azure-monitor/logs/basic-logs-configure#when-should-i-use-basic-logs) Azure Advisor will also recommend [changing pricing commitment tier](/azure/azure-monitor/logs/change-pricing-tier) for the whole workspace based on overall usage volume.

## Operational excellence

Operational Excellence primarily focuses on procedures for **development practices, observability, and release management**.

The [Operational Excellence design principles](../operational-excellence/principles.md) provide a high-level design strategy for achieving those goals towards the operational requirements of the workload.

### Design checklist

Start your design strategy based on the [**design review checklist for Operational Excellence**](../operational-excellence/checklist.md) for defining processes for observability, testing, and deployment related to Log Analytics workspaces.

> [!div class="checklist"]
> - **Ensure that operations staff is trained on Kusto Query Language (KQL)** and can create or modify queries when needed. If operators are unable to write or modify queries, it can slow down critical troubleshooting or other functions as they will have to rely on other teams to do that work for them.
> - **Use infrastructure-as-code (IaC) for all functions related to your workload's Log Analytics workspaces.** Minimize the risk of human error that can occur with manually administering and operating your log collection, ingestion, storage and querying functions, including saved queries and query packs, by automating as many of those functions as possible through code. In addition, include alerts that have been created to report health status changes and the configuration of diagnostic settings for resources that will be sending logs to your workspaces in your IaC code. Include this code with your other workload-related code to ensure that your safe deployment practices are maintained for the management of your workspaces.
> - **Ensure that your workspaces are healthy, and you are notified when issues arise.** Like any other component of your workload, your workspaces can encounter issues that can cost valuable time and resources to troubleshoot and resolve, and potentially leaving your team unaware of the production workload status. Being able to proactively monitor workspaces and mitigate potential issues will help your operations teams minimize the time they spend troubleshooting and fixing issues.
> - **Separate your production from nonproduction workloads.** Avoid unnecessary complexity that can cause extra work for operations team by using different workspaces for your production environment than those used by nonproduction environments. Comingled data can also lead to confusion as testing activities may appear to be events in production.
> - **Prefer built-in tools and functions over third-party solutions** to extend the functionality of your monitoring and logging systems. You may need to put additional configurations in place to support requirements like recoverability or data sovereignty that are not available out-of-the-box with Log Analytics workspaces. In these cases, whenever practical using native Azure or Microsoft tools will allow you to keep the number of tools that your organization must support to a minimum.
> - **Treat your workspaces as static rather than ephemeral components** Like other types of data stores, workspaces should not be considered among the ephemeral components of your workload. The Well-Architected Framework generally favors immutable infrastructure and the ability to quickly and easily replace resources within your workload as part of your deployments, but the loss of workspace data can be catastrophic and irreversible. For this reason, leave workspaces out of deployment packages that replace infrastructure during updates, and only perform in-place upgrades on the workspaces. 

### Recommendations

| Recommendation | Benefit |
|:---|:---|
| Design a workspace strategy to meet your business requirements.<br><br>See [Design a Log Analytics workspace architecture](/azure/azure-monitor/logs/workspace-design) for guidance on designing a strategy for your Log Analytics workspaces including how many to create and where to place them.<br><br>If your workload is required to use a centralized platform team offering ensure all necessary operational access is obtained and alerts can be constructed to ensure workload observability needs are met. | A single or at least minimal number of workspaces will maximize your workload's operational efficiency since it limits the distribution of your operational and security data, increasing your visibility into potential issues, making patterns easier to identify, and minimizing your maintenance requirements.<br><br>You may have requirements for multiple workspaces such as multiple tenants, or you may need workspaces in multiple regions to support your availability requirements. In these cases, ensure that you have appropriate processes in place to manage this increased complexity.|
| Use Infrastructure-as-code (IaC) to deploy and manage your workspaces and associated functions. | Use Infrastructure as Code (IaC) to define the details of your workspaces in [ARM](/azure/azure-monitor/logs/resource-manager-workspace), [BICEP](/azure/azure-monitor/logs/resource-manager-workspace), or [Terraform](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/log_analytics_workspace.html). This allows you to you leverage your existing DevOps processes to deploy new workspaces and [Azure Policy](/azure/governance/policy/overview) to enforce their configuration.<br><br> Co-locating all of you IaC code with your application code will help ensure that your safe deployment practices are maintained for all deployments. |
| Use Log Analytics workspace insights to track the health and performance of your Log Analytics workspaces, and create meaningful and actionable alerts to be proactively notified of operational issues.<br><br> [Log Analytics workspace insights](/azure/azure-monitor/logs/workspace-design) provides a unified view of the usage, performance, health, agents, queries, and change log for all your workspaces.<br><br> Each workspace has an [operation table](/azure/azure-monitor/logs/monitor-workspace) that logs important activities affecting workspace. | Review the information that Log Analytics insights provides on a regular basis to track the health and operation of each of your workspaces. Using this information will allow you to create easily understood visualizations like dashboards or reports that operations and stakeholders can use to track the health of your workspaces.<br><br> Create alert rules based on this table to be proactively notified when an operational issue occurs. You can use [recommended alerts for the workspace](/azure/azure-monitor/logs/log-analytics-workspace-health) to simplify the creation of the most critical alert rules. |
| Practice continuous improvement by frequently revisiting Azure diagnostic settings on your resources, data collection rules, and application log verbosity.<br><br>Ensure that you're optimizing your log collection strategy through frequent reviews of your resource settings. From an operational standpoint, look to reduce the noise in your logs by focusing on those logs that provide useful information about a resource’s health status.  | By optimizing in this manner, you’ll enable operators to more efficiently investigate and troubleshoot issues when they arise, or perform other routine, ad hoc, or emergency tasks.<br><br> When new diagnostic categories are made available for a resource type, review the types of logs that are emitted with this category to understand whether enabling them may help you optimize your collection strategy. For example, a new category may be a subset of a larger set of activities that are being captured and the new subset might allow you to reduce the volume of logs coming by focusing on the activities that are important for your operations to track. |

### Azure Policy and Azure Advisor

Azure offers no policies nor Azure Advisor recommendations related to the operational excellence of Log Analytics workspaces.

## Performance efficiency

Performance Efficiency is about **maintaining user experience even when there's an increase in load** by managing capacity. The strategy includes scaling resources, identifying and optimizing potential bottlenecks, and optimizing for peak performance.

The [Performance Efficiency design principles](../performance-efficiency/principles.md) provide a high-level design strategy for achieving those capacity goals against the expected usage.

### Design checklist

Start your design strategy based on the [**design review checklist for Performance Efficiency**](../performance-efficiency/checklist.md) for defining a baseline for your Log Analytics workspaces and associated functions.

> [!div class="checklist"]
> -**Be familiar with fundamentals of [log data ingestion latency in Azure Monitor](/azure/azure-monitor/logs/data-ingestion-time).** There are several factors that contribute to latency when ingesting logs into your workspaces and many of these factors are inherent to the Azure Monitor platform. Understanding those factors and the normal latency behavior can help you set appropriate expectations within your workload operations teams.
> - **Separate your nonproduction and production workloads.** Having production-specific workspaces will mitigate any overhead that nonproduction systems might introduce and reduce the overall footprint of your workspaces, requiring less resources to handle log data processing.
> - **Choose the right deployment regions to meet your performance requirements.** Deploy your Log Analytics workspace and data collection endpoints (DCEs) close to your workload. Your choice of the appropriate region to deploy your workspace and your DCEs in should be informed by your decision-making about where to deploy your workload.  You may need to weigh the performance benefits of deploying your workspaces and DCEs in the same region as your workload against your reliability requirements if you have already deployed your workload into a region that cannot support those requirements for your log data.

### Configuration recommendations

| Recommendation | Benefit |
|:---|:---|
| Configure log query auditing and use Log Analytics workspace insights to identify slow and inefficient queries. | [Log query auditing](/azure/azure-monitor/logs/query-audit) stores the compute time required to run each query and the time until results are returned. [Log Analytics workspace insights](/azure/azure-monitor/logs/log-analytics-workspace-insights-overview#query-audit-tab) uses this data to list potentially inefficient queries in your workspace. Consider rewriting these queries to improve their performance. Refer to [Optimize log queries in Azure Monitor](/azure/azure-monitor/logs/query-optimization) for guidance on optimizing your log queries. |
| Understand service limits for Log Analytics workspaces. | In certain high traffic implementations, you may run into service limits that affect your performance and will affect your workspace or workload design. For example, the query API limits the number of records and data volume returned by a query; the Logs ingestion API limits the size of each API call. See [Azure Monitor service limits](/azure/azure-monitor/service-limits#logs-ingestion-api) for a complete list of limits for Azure Monitor and [Log Analytics workspaces](/azure/azure-monitor/service-limits#log-analytics-workspaces) for limits specific to the workspace itself.  |
| Create DCR specific to data source type inside the defined observability scope(s). | Create separate DCRs for performance and events to optimize the backend processing compute utilization.<br><br> Not using different DCRs will force each and every associated virtual machine to transfer, process and execute configuration that might be not applicable according to the installed software. An excessive compute resource consumption and errors in processing configuration might happen causing the Azure Monitor Agent (AMA) becoming unresponsive. |

### Azure Policy and Azure Advisor

Azure offers no policies nor Azure Advisor recommendations related to the performance of Log Analytics workspaces.

## Next step

- [Get best practices for a complete deployment of Azure Monitor](/azure/azure-monitor/best-practices).
